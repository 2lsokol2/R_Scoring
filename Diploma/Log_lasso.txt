LASSO (least absolute shrinkage and selection operator - потужний метод оцінки коефіцієнтів регресії. Лассо є по суті методом регуляризації. Даний метод зменшує перенавчання, використовуючи менш складні функції . Один із способів зробити це - викинути менш важливі змінні після перевірки того, що вони не є важливими. Як ми обговорюватимемо пізніше, це можна зробити, вивчивши p-значення коефіцієнтів і відкинувши ті змінні, коефіцієнти яких не є значущими. Проте, це займає багато часу для проблем класифікації з багатьма незалежними змінними. У таких ситуаціях лассо пропонує спосіб моделювання залежної змінної при автоматичному виборі значущих змінних шляхом зменшення коефіцієнтів неважливих предикторів до нуля.
Як діє логістична регресія із використанням Лассо-регулізації?
Припустимо, що результат (передбачена змінна) і предиктори позначаються відповідно Y і X, і два класи, що представляють інтерес, позначаються + і - відповідно. Ми хочемо моделювати умовну ймовірність того, що результат Y дорівнює +, враховуючи, що вхідними змінними (предикторами) є X. Умовна ймовірність позначається p (Y = + | X), яку ми будемо скорочувати як p(X), оскільки ми знаємо, що ми маємо на увазі

ФОРМУЛА

Можна переконатися, що p(X) дійсно лежить між 0 і 1, оскільки X змінюється від - ∞ до + ∞. На малюнку також показано типову S-подібну криву, характерну для логістичної регресії.

КАРТИНКА

З’ясуємо звідки походить назва «логістична». Еквівалентним способом вираження наведеного рівняння є:

ФОРМУЛА

Значення ліворуч - логарифм шансів. Отже, модель являє собою лінійну регресію логарифмічних коефіцієнтів, іноді званий logit, і звідси назва логістична.
Проблема полягає в тому, щоб знайти значення beta_0 і beta_1, які призводять до p(X), що найбільш точно класифікує всі спостережувані точки даних - тобто ті, які належать до позитивного класу, мають ймовірність максимально наближеною до 1 і ті, що належать до негативного класу, мають ймовірність максимально близьку до 0. Один із способів сформулювати цю проблему:

ФОРМУЛА

Де представлені продукти над i та j, які проходять над + і - класифікованими точками відповідно. Такий підхід, який називається оцінкою максимальної правдоподібності, досить часто зустрічається у багатьох налаштуваннях машинного навчання, особливо тих, що стосуються ймовірностей.
Більше того, мінімізується ймовірність негативного логарифму правдоподібності, що, звичайно, є таким же, як максимізація ймовірності правдоподібності. Мінімізується таким чином:
ФОРМУЛА
Проте, це теоретичні деталі, які я згадую тільки для повноти. Як ви побачите далі, вони мало впливають на практичне використання логістичної регресії із лассо регулізацією.
Логістична регресія в R 
У цьому прикладі ми будемо використовувати метод логістичної регресії, реалізований у функції glm, яка постачається з базовою інсталяцією R. Ця функція відповідає класу моделей, загальновідомих як узагальнені лінійні моделі. 
КОД

Хоча все виглядає досить добре, але є проблема, яка ховається «всередині». Щоб побачити це, давайте розглянемо інформацію, отриману в результаті опису моделі, зокрема оцінки коефіцієнтів (тобто оцінки для бета-версії) та їх значення. Нижче наведено резюме інформації, що міститься в таблиці:
У колонці 2 таблиці наведені оцінки коефіцієнтів.
У стовпці 3 наведено стандартну помилку оцінок (чим більша стандартна помилка, тим менше ми впевнені в оцінці)
Колонка 4 статистики z (яка є оцінкою коефіцієнтів (колонка 2), поділена на стандартну помилку оцінки (колонка 3)) і
Останній стовпець (Pr (> | z |)) перераховує p-значення, яке є ймовірністю отримання зазначеної оцінки, вважаючи, що предиктор не має ефекту.
№№№№№№№№№редаг
З таблиці можна зробити висновок, що тільки  предиктора є значущими - (і, можливо, п'яте - тиск). Інші змінні мають малу прогностичну силу і гірше, можуть сприяти перенавчанню. Тому вони повинні бути усунені. Проте, перш ніж ми це зробимо, необхідно відзначити важливий момент.
У цьому випадку ми маємо лише 9 змінних, тому здатні ідентифікувати значні за допомогою ручної перевірки p-значень. Як ви можете собі уявити, такий процес швидко стане нудним, оскільки кількість змінних зростає. Хіба не було б приємно, якщо б існував алгоритм, який може якось автоматично зменшити коефіцієнти цих змінних? Виявляється, це саме те, що роблять лассо і її «двоюрідний брат» - рідж регресія.
Ridge і Lasso регресія
Нагадаємо, що значення коефіцієнтів логістичної регресії beta_0 і beta_1 виявляються шляхом мінімізації ф-ції правдоподібності ,описаного в рівнянні (3). Рідж і лассо регуляризація працюють шляхом додавання штрафу на функцію правдоподібності. У разі рідж регресії термін штрафу – це , а у випадку ласо - це  .Таким чином, кількість мінімізованих у двох випадках:
ФОРМУЛИ
Де лямбда - це вільний параметр, який зазвичай вибирається таким чином, щоб отримана модель мінімізувала помилку вибірки. Більшість консервованих алгоритмів надають методи для цього; той, який ми будемо використовувати в наступному розділі, не є винятком.
У разі рідж регресії ефект регулізації полягає в зменшенні коефіцієнтів, які найбільше впливають на помилку моделі. Іншими словами, це зменшує величину коефіцієнтів, що сприяють збільшенню L. На відміну від цього, у випадку регресії лассо, ефект штрафу проявляється у зменшенні цих коефіцієнтів практично до нуля! Це круто, тому що це означає, що регресія лассо працює як селектор змінних, тобто вибирає найважливіші змінні, ті, які мають найбільшу прогнозтичну силу(мають найменші значення p).
Проілюструємо це на прикладі. Ми будемо використовувати пакет glmnet, який реалізує комбіновану версію ріжд та ласо (називається еластичною сіткою). Замість мінімізації (4) або (5) вище, glmnet мінімізує:
ФОРМУЛА
де альфа керує «сумішшю» регуляризації рідж і лассо, причому альфа = 0 є «чистим» рідж і альфа = 1 є «чистим» лассо.

Лассо - регуляризація з використанням glmnet
Давайте повторно аналізуємо набір даних індійського діабету Піма, використовуючи glmnet з альфа = 1 (чисте ласо). Перед тим, як поринути в код, варто відзначити, що glmnet:

не має інтерфейсу формули, тому потрібно ввести предиктори як матрицю і мітки класу як вектор.
не приймає категоричних предикторів, тому необхідно перевести їх у числові значення, перш ніж передати їх у glmnet.
Функція glmnet model.matrix створює матрицю і також перетворює категоріальні предиктори у відповідні фіктивні змінні.

Іншим важливим моментом є те, що ми будемо використовувати функцію cv.glmnet, яка автоматично виконує пошук сітки, щоб знайти оптимальне значення лямбда.

Добре, досить сказав, ми йдемо:

МНОГО ФОРМУЛ


Діаграма показує, що log оптимального значення лямбда (тобто той, який мінімізує середньоквадратичну помилку), становить приблизно -5. Точне значення можна переглянути, розглянувши змінну lambda_min в коді нижче. Взагалі, метою регуляризації є збалансування точності та простоти. У даному контексті це означає модель з найменшим числом коефіцієнтів, що також дає хорошу точність. Для цього функція cv.glmnet знаходить значення лямбда, що дає найпростішу модель, але також лежить в межах однієї стандартної помилки оптимального значення лямбда. 
Результати показують, що тільки ті змінні, які ми визначили значними на основі p-значень, мають ненульові коефіцієнти. Коефіцієнти всіх інших змінних дорівнюють нулю. Переконаємося як саме це вплине на точність. Давайте побачимо, запустивши модель у відповідності до наших тестових даних:
Ми розглянули оптимізацію логістичної регресії, одну з найпростіших (і найстаріших) методів класифікації в арсеналі практикуючих методів. Незважаючи на простоту (або, треба сказати, через це), логістична регресія добре працює для багатьох бізнес-проблем, які часто мають прості рішення. Більш того, через його простоту він менш схильний до перенавчання, ніж методи, побудовані на деревах рішень. Але даний метод можна оптимізувати усуненням змінних, які сприяють перенавчанню, змінні будуть усунені за допомогою лассо регуляризації, без погіршення точності. Враховуючи ці переваги та властиву їй простоту, не дивно, що логістична регресія залишається одним із найпопулярніших методів для скоронгових моделей.
